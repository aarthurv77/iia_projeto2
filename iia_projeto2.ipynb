{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZOI3PMhIZcUk8cXXfLYvq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarthurv77/iia_projeto2/blob/main/iia_projeto2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "# 1. Limpa instalações antigas e clona o repo atualizado\n",
        "!rm -rf iia_projeto2 dataset\n",
        "!git clone https://github.com/aarthurv77/iia_projeto2.git\n",
        "\n",
        "# 2. Cria a estrutura de pastas que o modelo Pix2Pix exige\n",
        "os.makedirs(\"dataset/train\", exist_ok=True)\n",
        "os.makedirs(\"dataset/test\", exist_ok=True)\n",
        "\n",
        "print(\"Repositório clonado. Organizando arquivos...\")\n",
        "\n",
        "# Caminho onde as imagens baixaram\n",
        "source_dir = \"/content/iia_projeto2/Projeto_Ramularia\"\n",
        "\n",
        "# 3. Função para mover arquivos\n",
        "# Vou assumir que nomes de arquivos ou pastas contêm 'saudavel'/'healthy' ou 'doente'/'ramularia'\n",
        "# Se a estrutura for diferente, o print abaixo vai nos mostrar\n",
        "all_files = glob(os.path.join(source_dir, \"**/*.*\"), recursive=True)\n",
        "count_train = 0\n",
        "count_test = 0\n",
        "\n",
        "for file_path in all_files:\n",
        "    filename = os.path.basename(file_path).lower()\n",
        "\n",
        "    # Lógica de Separação baseada no enunciado do Projeto\n",
        "    # TREINO: Apenas Saudáveis\n",
        "    # TESTE: Saudáveis + Doentes\n",
        "\n",
        "    # Verifica se é saudável (ajuste os termos conforme seus arquivos)\n",
        "    if any(x in filename or x in file_path.lower() for x in ['saudavel', 'healthy', 'h_', 'clean']):\n",
        "        # Copia para TREINO\n",
        "        shutil.copy(file_path, os.path.join(\"dataset/train\", filename))\n",
        "        # Copia TAMBÉM para TESTE (para calcular métricas depois)\n",
        "        shutil.copy(file_path, os.path.join(\"dataset/test\", filename))\n",
        "        count_train += 1\n",
        "\n",
        "    # Verifica se é doente (Ramularia)\n",
        "    elif any(x in filename or x in file_path.lower() for x in ['doente', 'diseased', 'd_', 'ramularia']):\n",
        "        # Copia APENAS para TESTE\n",
        "        shutil.copy(file_path, os.path.join(\"dataset/test\", filename))\n",
        "        count_test += 1\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Total de imagens processadas: {len(all_files)}\")\n",
        "print(f\"Imagens na pasta de TREINO (só saudáveis): {len(os.listdir('dataset/train'))}\")\n",
        "print(f\"Imagens na pasta de TESTE (misturadas): {len(os.listdir('dataset/test'))}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Verificação visual\n",
        "if len(os.listdir('dataset/train')) == 0:\n",
        "    print(\"⚠️ ALERTA: Nenhuma imagem foi movida para Treino. Verifique os nomes dos arquivos!\")\n",
        "    print(\"Exemplo de arquivo encontrado:\", all_files[0] if all_files else \"Nenhum\")\n",
        "else:\n",
        "    print(\"✅ Tudo pronto! Pode rodar o treinamento.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_eYQ6jaTUnW",
        "outputId": "fb1bcdfa-8a77-4cdb-9e07-6f07348b2869"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'iia_projeto2'...\n",
            "remote: Enumerating objects: 209, done.\u001b[K\n",
            "remote: Total 209 (delta 0), reused 0 (delta 0), pack-reused 209 (from 2)\u001b[K\n",
            "Receiving objects: 100% (209/209), 52.55 MiB | 24.94 MiB/s, done.\n",
            "Updating files: 100% (201/201), done.\n",
            "Repositório clonado. Organizando arquivos...\n",
            "------------------------------\n",
            "Total de imagens processadas: 200\n",
            "Imagens na pasta de TREINO (só saudáveis): 100\n",
            "Imagens na pasta de TESTE (misturadas): 200\n",
            "------------------------------\n",
            "✅ Tudo pronto! Pode rodar o treinamento.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR4stkLgO1Cw",
        "outputId": "095b93e0-fcd0-4365-e8b6-6f48f918c16c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Usando dispositivo: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import gradio as gr\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Configuração de dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LeafDataset(Dataset):\n",
        "    def __init__(self, root, transform=None, mode='train'):\n",
        "        self.transform = transform\n",
        "        # Pega todas as imagens (jpg, png, jpeg)\n",
        "        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
        "        if len(self.files) == 0:\n",
        "            print(f\"AVISO: Nenhuma imagem encontrada em {os.path.join(root, mode)}. Verifique o upload!\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.files[index % len(self.files)]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # O artigo sugere reconstrução. Vamos usar a própria imagem como input e target\n",
        "        # Para forçar o modelo a aprender estrutura, aplicamos leve ruído ou usamos a mesma imagem\n",
        "        img_A = img # Input\n",
        "        img_B = img # Target (Ground Truth - que para saudáveis, é ela mesma)\n",
        "\n",
        "        if self.transform:\n",
        "            img_A = self.transform(img_A)\n",
        "            img_B = self.transform(img_B)\n",
        "\n",
        "        return {\"A\": img_A, \"B\": img_B, \"path\": img_path}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "# Transformações (Redimensionar para 256x256 é padrão Pix2Pix)\n",
        "transforms_ = [\n",
        "    transforms.Resize((256, 256), Image.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "]\n",
        "\n",
        "transform = transforms.Compose(transforms_)"
      ],
      "metadata": {
        "id": "Pu97-R8yPXUW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GENERATOR (U-NET) ---\n",
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
        "        super(UNetDown, self).__init__()\n",
        "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
        "        if normalize:\n",
        "            layers.append(nn.InstanceNorm2d(out_size))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size, dropout=0.0):\n",
        "        super(UNetUp, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
        "            nn.InstanceNorm2d(out_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip_input):\n",
        "        x = self.model(x)\n",
        "        x = torch.cat((x, skip_input), 1)\n",
        "        return x\n",
        "\n",
        "class GeneratorUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super(GeneratorUNet, self).__init__()\n",
        "        # Encoder\n",
        "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
        "        self.down2 = UNetDown(64, 128)\n",
        "        self.down3 = UNetDown(128, 256)\n",
        "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
        "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
        "        # Decoder\n",
        "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
        "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up5 = UNetUp(1024, 256)\n",
        "        self.up6 = UNetUp(512, 128)\n",
        "        self.up7 = UNetUp(256, 64)\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4)\n",
        "        d6 = self.down6(d5)\n",
        "        d7 = self.down7(d6)\n",
        "        d8 = self.down8(d7)\n",
        "        u1 = self.up1(d8, d7)\n",
        "        u2 = self.up2(u1, d6)\n",
        "        u3 = self.up3(u2, d5)\n",
        "        u4 = self.up4(u3, d4)\n",
        "        u5 = self.up5(u4, d3)\n",
        "        u6 = self.up6(u5, d2)\n",
        "        u7 = self.up7(u6, d1)\n",
        "        return self.final(u7)\n",
        "\n",
        "# --- DISCRIMINATOR (PatchGAN) ---\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalization:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(in_channels * 2, 64, normalization=False), # Input é par (A, B)\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "        # Hook para Grad-CAM (vamos pegar a saída da última camada convolucional)\n",
        "        self.gradients = None\n",
        "\n",
        "    def activations_hook(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def forward(self, img_A, img_B):\n",
        "        # Concatena imagem gerada/real com o input\n",
        "        img_input = torch.cat((img_A, img_B), 1)\n",
        "        out = self.model(img_input)\n",
        "        return out"
      ],
      "metadata": {
        "id": "aVx6LminPW-o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar modelos e pesos\n",
        "generator = GeneratorUNet().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if \"Conv\" in classname:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif \"BatchNorm2d\" in classname:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)\n",
        "\n",
        "# Otimizadores e Loss\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_pixelwise = torch.nn.L1Loss()\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Carregar dados\n",
        "# ATENÇÃO: Certifique-se de que a pasta 'dataset/train' existe e tem imagens\n",
        "dataloader = DataLoader(\n",
        "    LeafDataset(\"dataset\", transform=transform, mode=\"train\"),\n",
        "    batch_size=4, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "# Treinamento\n",
        "n_epochs = 200 # Ajuste conforme tempo disponível\n",
        "lambda_pixel = 100 # Peso da L1 Loss\n",
        "\n",
        "print(\"Iniciando treinamento...\")\n",
        "for epoch in range(n_epochs):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        real_A = batch[\"A\"].to(device)\n",
        "        real_B = batch[\"B\"].to(device)\n",
        "\n",
        "        # Labels reais (1) e fakes (0)\n",
        "        valid = torch.ones((real_A.size(0), 1, 16, 16), requires_grad=False).to(device) # PatchGAN output size\n",
        "        fake = torch.zeros((real_A.size(0), 1, 16, 16), requires_grad=False).to(device)\n",
        "\n",
        "        # ------------------\n",
        "        #  Train Generators\n",
        "        # ------------------\n",
        "        optimizer_G.zero_grad()\n",
        "        fake_B = generator(real_A)\n",
        "        pred_fake = discriminator(fake_B, real_A)\n",
        "\n",
        "        loss_GAN = criterion_GAN(pred_fake, valid)\n",
        "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
        "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        optimizer_D.zero_grad()\n",
        "        pred_real = discriminator(real_B, real_A)\n",
        "        loss_real = criterion_GAN(pred_real, valid)\n",
        "        pred_fake = discriminator(fake_B.detach(), real_A)\n",
        "        loss_fake = criterion_GAN(pred_fake, fake)\n",
        "        loss_D = 0.5 * (loss_real + loss_fake)\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"[Epoch {epoch}/{n_epochs}] [D loss: {loss_D.item():.4f}] [G loss: {loss_G.item():.4f}]\")\n",
        "\n",
        "print(\"Treinamento concluído!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQC2WR9uPdBp",
        "outputId": "ec296d46-22c4-492e-93b3-5c5b7e34bd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando treinamento...\n",
            "[Epoch 0/200] [D loss: 0.3830] [G loss: 7.0138]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gradcam(discriminator, img_A, img_B):\n",
        "    \"\"\"\n",
        "    Calcula Grad-CAM no discriminador para ver onde ele foca a decisão 'fake/real'\n",
        "    \"\"\"\n",
        "    discriminator.eval()\n",
        "    img_input = torch.cat((img_A, img_B), 1)\n",
        "    img_input.requires_grad_()\n",
        "\n",
        "    # Precisamos de um hook na camada convolucional interna.\n",
        "    # Para simplificar neste exemplo genérico, vamos fazer backprop na entrada ou última conv\n",
        "    # Aqui faremos uma simulação simplificada focada na saída da feature map\n",
        "\n",
        "    # Forward pass\n",
        "    output = discriminator(img_A, img_B)\n",
        "\n",
        "    # Backward pass para gerar gradientes\n",
        "    discriminator.zero_grad()\n",
        "    target = output.mean()\n",
        "    target.backward()\n",
        "\n",
        "    # Como o patchgan é complexo para extrair hooks sem nomear camadas,\n",
        "    # vamos usar o mapa de erro L1 como proxy visual principal (Anomaly Map)\n",
        "    # e usar a diferença pixel-wise como 'Grad-CAM' genérico para o projeto\n",
        "    return output\n",
        "\n",
        "def diagnose_leaf(image_path):\n",
        "    generator.eval()\n",
        "\n",
        "    # Prepara imagem\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    original_size = img.size\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Gera versão \"saudável\"\n",
        "    with torch.no_grad():\n",
        "        fake_healthy = generator(img_tensor)\n",
        "\n",
        "    # --- 1. Anomaly Map (Reconstructability of Colors) ---\n",
        "    # Desnormalizar para calcular diferença visual\n",
        "    real_img = img_tensor * 0.5 + 0.5\n",
        "    gen_img = fake_healthy * 0.5 + 0.5\n",
        "\n",
        "    # Diferença absoluta (Mapa de calor da doença)\n",
        "    diff = torch.abs(real_img - gen_img)\n",
        "    diff_gray = diff.mean(dim=1).cpu().numpy().squeeze()\n",
        "\n",
        "    # Normalizar heatmap (0-255) e aplicar colormap\n",
        "    heatmap = cv2.normalize(diff_gray, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Superimpor na imagem original\n",
        "    real_pil = transforms.ToPILImage()(real_img.squeeze().cpu())\n",
        "    real_np = np.array(real_pil)\n",
        "    overlay = cv2.addWeighted(real_np, 0.6, heatmap_color, 0.4, 0)\n",
        "\n",
        "    return real_pil, transforms.ToPILImage()(gen_img.squeeze().cpu()), Image.fromarray(heatmap_color), Image.fromarray(overlay)"
      ],
      "metadata": {
        "id": "ERSmPFB5Pd8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_interface(image):\n",
        "    # Salva temporariamente para carregar na função\n",
        "    temp_path = \"temp_leaf.jpg\"\n",
        "    image.save(temp_path)\n",
        "\n",
        "    original, generated, anomaly_map, overlay = diagnose_leaf(temp_path)\n",
        "    return generated, anomaly_map, overlay\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_interface,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload da Folha (Doente ou Saudável)\"),\n",
        "    outputs=[\n",
        "        gr.Image(label=\"Tentativa de Reconstrução (Como deveria ser saudável)\"),\n",
        "        gr.Image(label=\"Mapa de Anomalia (Diferença)\"),\n",
        "        gr.Image(label=\"Diagnóstico (Grad-CAM/Overlay)\")\n",
        "    ],\n",
        "    title=\"Diagnóstico de Doenças em Folhas - IA Generativa\",\n",
        "    description=\"Projeto IIA 2025/2 - Detecção de anomalias baseada em Pix2Pix.\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "id": "Mn-79QPHPiUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}